# How do goroutines differ from OS threads?

**What Are Goroutines?**
Goroutines are lightweight, concurrent execution units managed by the Go runtime. Conceptually similar to threads, goroutines are cheaper to create, schedule, and maintain, thanks to Go’s user-space scheduler.
```go
go someFunction() // Spawns a new goroutine
```
**What Are OS Threads?**
OS threads are managed by the operating system’s kernel. Each thread typically has a large fixed-size stack and incurs overhead during creation, destruction, and context switching.

## Key Technical Differences

### 1. Resource Efficiency
Goroutines start with a very small stack (typically 2 KB) that grows/shrinks dynamically.

OS threads typically allocate a large fixed stack (e.g., 1 MB), even if most of it isn’t used.

Implication: You can run millions of goroutines on a machine, but only thousands of threads before running out of memory.

### 2. Scheduling and Execution
Goroutines are scheduled by the Go runtime’s M:N scheduler, which multiplexes many goroutines (G) onto a smaller number of OS threads (M) using logical processors (P).

#### The scheduler uses a work-stealing algorithm:

- Each logical processor (P) maintains its own local queue of goroutines.
- If one P runs out of goroutines, it can steal work from another P's queue, ensuring better CPU utilization and load balancing.
- OS threads are scheduled preemptively by the kernel, which uses general-purpose policies like round-robin, priority queues, etc.

Implication: Goroutines offer better control and less overhead in high-concurrency environments. However, OS threads may be better in real-time systems where preemption and priority are critical.

### 3. Context Switching
Goroutine switches are faster since they happen in user space and involve no kernel-mode transition.

Thread context switches are more expensive: CPU registers, stack pointers, and memory mappings must be saved/restored by the kernel.

Example: In a network server, goroutines can handle thousands of connections without incurring high context-switching overhead.

### 4. Blocking and I/O
When a goroutine blocks on I/O (e.g., network call), Go runtime parks the goroutine and moves the OS thread to another runnable goroutine.

In contrast, if an OS thread blocks on I/O, the entire thread becomes unavailable until the I/O completes.

Implication: Goroutines support non-blocking concurrency by default with simpler syntax, avoiding callback hell or complex async/await trees.

## Go Code Example

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

func worker(id int) {
    for i := 0; i < 3; i++ {
        fmt.Printf("Worker %d is working on task %d\n", id, i)
        time.Sleep(100 * time.Millisecond)
    }
}

func main() {
    runtime.GOMAXPROCS(2) // Set number of processors (P) to 2

    for i := 0; i < 5; i++ {
        go worker(i) // Start 5 goroutines (G)
    }

    time.Sleep(1 * time.Second)
}
```

### What Happens at Runtime
- GOMAXPROCS(2) tells Go to use 2 logical processors (P).
- We spawn 5 goroutines.

#### The Go scheduler:

- Distributes goroutines across local queues owned by each processor.
- Assigns available OS threads (M) to execute goroutines in the queues.
- If one processor (P) runs out of tasks and another is still busy, it steals a goroutine from the other’s queue — that's work-stealing.

Output will interleave, but goroutines will be executed concurrently, not in strict order.

Mapping to Execution Model
Component	In Code	Meaning

- Goroutines (G)	go worker(i)	Tasks to execute
- Processors (P)	runtime.GOMAXPROCS(2)	Logical CPU slots holding run queues
- Threads (M)	Implicit	OS threads executing the goroutines

![image](https://github.com/user-attachments/assets/983fd5e6-2072-49ad-af6a-ceb499f5b05c)



### References
- https://betterprogramming.pub/memory-optimization-and-garbage-collector-management-in-go-71da4612a960
- https://bwoff.medium.com/understanding-gos-garbage-collection-415a19cc485c
