# Design a Web Crawler 

The core functionality of a web crawler involves fetching web pages, parsing their content and metadata, and extracting new URLs or lists of URLs for further crawling. 

## LLD

<details>

### 1. Factory Pattern (for HTTP client)
Why? So you can create different types of HTTP clients dynamically depending on need.

Real-world scenario:
Letâ€™s say:

You want one HTTP client that has retry logic for flaky sites.

Another thatâ€™s optimized for speed and skips redirects.

Using a Factory, you can abstract away the construction logic and create the right client on the fly, without changing the crawler core logic.

```go
client := httpClientFactory("retryable")  // returns a custom-configured client
```

### 2. Strategy Pattern (for parser)
Why? To support different parsing strategies for different content types or websites.

Real-world scenario:
Some pages are HTML, some are XML APIs, some use JavaScript rendering.

By using the Strategy Pattern, you can plug in a different parsing method at runtime:

```go
htmlParser := NewHTMLParser()
jsonParser := NewJSONParser()

crawler.SetParser(htmlParser)
```
This avoids writing spaghetti if contentType == ... logic everywhere.

### 3. Observer Pattern (to notify when a URL is processed)
Why? To decouple processing events from response handling.

Real-world scenario:
You want to:

Log every crawled URL to a file

Send a webhook notification

Update a UI dashboard in real time

Using Observer Pattern, you can register multiple listeners like:

```go
crawler.Subscribe(logger)
crawler.Subscribe(dashboardNotifier)
```
Now, all are notified without changing your crawler logic.

### 4. Command Pattern (encapsulating crawl request)
Why? To wrap crawling actions as independent command objects.

ðŸ’¡Real-world scenario:
You might want to:

Retry failed requests later

Queue crawl jobs for auditing

Serialize/deserialize commands for distributed crawling

Instead of just calling crawl(url), you encapsulate it:

```go
command := NewCrawlCommand(url)
command.Execute()
```
Now you have flexibility to store, retry, or extend behavior easily.

<details>
<summary> Code Implementation</summary>  
```go

// web_crawler.go
package main

import (
	"fmt"
	"net/http"
	"net/url"
	"sync"
	"time"

	"golang.org/x/net/html"
)

// ----------- Singleton URL Queue (Thread-safe) ------------
type URLQueue struct {
	queue chan string
}

var (
	once     sync.Once
	singleQ *URLQueue
)

func GetURLQueue() *URLQueue {
	once.Do(func() {
		singleQ = &URLQueue{
			queue: make(chan string, 1000),
		}
	})
	return singleQ
}

func (q *URLQueue) Enqueue(url string) {
	q.queue <- url
}

func (q *URLQueue) Dequeue() string {
	return <-q.queue
}

// -------------- HTTP Client Factory ------------------
type HTTPClientFactory struct{}

func (f *HTTPClientFactory) CreateClient() *http.Client {
	return &http.Client{Timeout: 10 * time.Second}
}

// -------------- Parser Strategy Pattern --------------
type ParserStrategy interface {
	Parse(resp *http.Response) ([]string, error)
}

type HTMLLinkParser struct{}

func (p *HTMLLinkParser) Parse(resp *http.Response) ([]string, error) {
	links := []string{}
	tokens := html.NewTokenizer(resp.Body)
	for {
		tt := tokens.Next()
		if tt == html.ErrorToken {
			break
		}
		token := tokens.Token()
		if token.Type == html.StartTagToken && token.Data == "a" {
			for _, attr := range token.Attr {
				if attr.Key == "href" {
					absURL := resolveURL(resp.Request.URL.String(), attr.Val)
					if absURL != "" {
						links = append(links, absURL)
					}
				}
			}
		}
	}
	return links, nil
}

func resolveURL(base, ref string) string {
	b, err1 := url.Parse(base)
	r, err2 := url.Parse(ref)
	if err1 != nil || err2 != nil {
		return ""
	}
	return b.ResolveReference(r).String()
}

// -------------- Observer Pattern ------------------
type URLObserver interface {
	Notify(url string)
}

type LoggerObserver struct{}

func (l *LoggerObserver) Notify(url string) {
	fmt.Println("[Crawled]", url)
}

// -------------- Command Pattern ------------------
type CrawlCommand struct {
	url      string
	client   *http.Client
	parser   ParserStrategy
	observers []URLObserver
	rateLimiter <-chan time.Time
	visited  *sync.Map
	queue    *URLQueue
	wg       *sync.WaitGroup
}

func (cmd *CrawlCommand) Execute() {
	defer cmd.wg.Done()
	<-cmd.rateLimiter // politeness

	if _, seen := cmd.visited.LoadOrStore(cmd.url, true); seen {
		return
	}

	resp, err := cmd.client.Get(cmd.url)
	if err != nil {
		return
	}
	defer resp.Body.Close()

	links, err := cmd.parser.Parse(resp)
	if err != nil {
		return
	}

	for _, o := range cmd.observers {
		o.Notify(cmd.url)
	}

	for _, link := range links {
		cmd.wg.Add(1)
		go func(l string) {
			cmd.queue.Enqueue(l)
		}(link)
	}
}

// ------------------ Main Crawler Logic ------------------
func main() {
	startURL := "https://example.com"
	clientFactory := &HTTPClientFactory{}
	client := clientFactory.CreateClient()
	parser := &HTMLLinkParser{}
	logger := &LoggerObserver{}

	rateLimiter := time.Tick(500 * time.Millisecond) // politeness policy
	queue := GetURLQueue()
	queue.Enqueue(startURL)

	visited := &sync.Map{}
	var wg sync.WaitGroup

	for i := 0; i < 5; i++ {
		go func() {
			for {
				url := queue.Dequeue()
				cmd := &CrawlCommand{
					url: url,
					client: client,
					parser: parser,
					observers: []URLObserver{logger},
					rateLimiter: rateLimiter,
					visited: visited,
					queue: queue,
					wg: &wg,
				}
				wg.Add(1)
				cmd.Execute()
			}
		}()
	}

	wg.Wait()
}
```

</details>
</details>

## HLD System design

### Challenges of a Web Crawler System Design
While designing a web crawler, several challenges arise:

- Crawler traps: Infinite loops caused by dynamic links or calendar pages.
- Duplicate content: Crawling the same web pages repeatedly wastes resources.
- Rate limiting: Fetching too many pages from a single domain can lead to server overload. We need load balancing to balance the loads on web servers or application servers.
- DNS lookup latency: Frequent domain name system (DNS) lookups increase latency.
- Scalability: Handling large-scale crawling is challenging and demands a distributed system that can process millions of seed URLs and distribute load across multiple web servers.


